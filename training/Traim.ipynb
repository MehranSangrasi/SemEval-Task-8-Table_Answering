{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from peft) (6.1.0)\n",
      "Requirement already satisfied: pyyaml in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from peft) (2.5.1)\n",
      "Requirement already satisfied: transformers in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from peft) (4.46.3)\n",
      "Requirement already satisfied: tqdm in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from peft) (4.67.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from peft) (1.1.1)\n",
      "Requirement already satisfied: safetensors in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from peft) (0.26.2)\n",
      "Requirement already satisfied: filelock in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from huggingface-hub>=0.25.0->peft) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from huggingface-hub>=0.25.0->peft) (2024.9.0)\n",
      "Requirement already satisfied: requests in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (75.5.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from transformers->peft) (0.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.8.30)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting scipy (from bitsandbytes)\n",
      "  Using cached scipy-1.14.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<2.3,>=1.23.5 in /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages (from scipy->bitsandbytes) (2.0.2)\n",
      "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached scipy-1.14.1-cp312-cp312-macosx_14_0_arm64.whl (23.1 MB)\n",
      "Installing collected packages: scipy, bitsandbytes\n",
      "Successfully installed bitsandbytes-0.42.0 scipy-1.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install peft\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")  # Set MPS as the device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRa Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,  \n",
    "    lora_alpha=32,  \n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.05,  \n",
    "    bias=\"none\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:58<00:00, 14.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"arcee-ai/Llama-3.1-SuperNova-Lite\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying and Verifying LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "CUDA SETUP: Loading binary /Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "dlopen(/Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_cpu.so, 0x0006): tried: '/Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (no such file), '/Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('csv', data_files=\"../data/queries_final.csv\")\n",
    "# val_dataset = load_dataset(\"path_to_val_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset['train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"df.loc[df['finalWorth'].idxmax(), 'selfMade']\",\n",
       " \"df.loc[df['age'].idxmin(), 'gender'] == 'male'\",\n",
       " \"df['city'].value_counts().idxmax() == 'United States'\",\n",
       " \"df.nlargest(5, 'rank')['selfMade'].eq(False).any()\",\n",
       " \"df.loc[df['age'] == df['age'].max(), 'philanthropyScore'].iloc[0] == 5\",\n",
       " \"df['age'].min()\",\n",
       " \"df[df['category'] == 'Technology'].shape[0]\",\n",
       " \"df[df['category'] == 'Automotive']['finalWorth'].sum()\",\n",
       " \"df[df['philanthropyScore'] > 3].shape[0]\",\n",
       " \"df.loc[(df['selfMade'] == False), 'rank'].idxmax()\",\n",
       " \"df.loc[df['finalWorth'] == df['finalWorth'].max(), 'category'].iloc[0]\",\n",
       " \"df.loc[df['age'] == df['age'].max(), 'country'].iloc[0]\",\n",
       " \"df.loc[df['philanthropyScore'] == df['philanthropyScore'].max(), 'gender'].iloc[0]\",\n",
       " \"df.loc[df['age'] == df['age'].min(), 'source'].iloc[0]\",\n",
       " \"df.loc[df['rank'].idxmin(), 'title']\",\n",
       " \"df['country'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['source'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df.nsmallest(4, 'age')['city'].tolist()\",\n",
       " \"df['category'].value_counts().nsmallest(3).index.tolist()\",\n",
       " \"df['country'].value_counts().nsmallest(2).index.tolist()\",\n",
       " \"df[df['selfMade'] == False].nsmallest(5, 'rank')['rank'].tolist()\",\n",
       " \"df[df['philanthropyScore'] == 5].nsmallest(3, 'age')['age'].tolist()\",\n",
       " \"df[df['category'] == 'Technology'].nlargest(6, 'finalWorth')['finalWorth'].tolist()\",\n",
       " \"df[df['gender'] == 'female'].nsmallest(4, 'rank')['rank'].tolist()\",\n",
       " \"df[df['category'] == 'Automotive'].nlargest(2, 'finalWorth')['finalWorth'].tolist()\",\n",
       " \"df[(df['Age'] < 18) & (df['Survived'] == True)].any()\",\n",
       " \"df['Fare'].gt(500).any()\",\n",
       " \"df['Name'].is_unique\",\n",
       " \"df[(df['Pclass'] == 3) & (df['Sex'] == 'female') & (df['Survived'] == True)].any()\",\n",
       " \"df['Pclass'].nunique()\",\n",
       " \"df['Age'].max()\",\n",
       " \"df[df['Siblings_Spouses Aboard'] == 0].shape[0]\",\n",
       " \"df['Fare'].mean()\",\n",
       " \"df[df['Survived'] == True]['Pclass'].value_counts().idxmax()\",\n",
       " \"df[df['Survived'] == True]['Sex'].mode()[0]\",\n",
       " \"df[df['Survived'] == True]['Fare'].apply(lambda x: '0-50' if x <= 50 else '50-100' if x <= 100 else '100-150' if x <= 150 else '150+').value_counts().idxmax()\",\n",
       " \"df['Age'].apply(lambda x: '(0-18)' if x <= 18 else '(18-30)' if x <= 30 else '(30-50)' if x <= 50 else '(50+)').mode().iloc[0]\",\n",
       " \"df.groupby('Pclass')['Survived'].mean().nlargest(3).index.tolist()\",\n",
       " \"df.groupby(pd.cut(df['Fare'], bins=[0, 50, 100, 150, float('inf')])).agg({'Survived': 'sum'}).nsmallest(3, 'Survived').index.tolist()\",\n",
       " \"df.groupby(pd.cut(df['Age'], bins=[-1, 18, 30, 50, float('inf')], labels=['0-18', '18-30', '30-50', '50+']))['Survived'].sum().nlargest(4).index.tolist()\",\n",
       " \"df.groupby('Sex')['Fare'].mean().nlargest(2).index.tolist()\",\n",
       " \"df[df['Survived'] == True]['Age'].nlargest(3).tolist()\",\n",
       " \"df[df['Survived'] == True].nlargest(4, 'Fare')['Fare'].tolist()\",\n",
       " \"df[df['Survived'] == True]['Age'].nsmallest(3).tolist()\",\n",
       " \"df[df['Survived'] == False].nsmallest(4, 'Fare')['Fare'].tolist()\",\n",
       " \"df['What is your age? 👶🏻👵🏻'].mean() > 30\",\n",
       " \"df['What is your civil status? 💍'].value_counts()['Single'] > df['What is your civil status? 💍'].value_counts()['Married']\",\n",
       " \"df['What's your height? in cm 📏'].mean() > 170\",\n",
       " \"df['hair color'].value_counts().idxmax() == 'black'\",\n",
       " \"df['What\\\\'s your nationality?'].nunique()\",\n",
       " \"df['Gross annual salary (in euros) 💸'].mean()\",\n",
       " \"df[df['How often do you wear glasses? 👓'] == 'All the time'].shape[0]\",\n",
       " \"df['What is your age? 👶🏻👵🏻'].median()\",\n",
       " \"df['What is the maximum level of studies you have achieved? 🎓'].mode()[0]\",\n",
       " \"df['What is your body complexity? 🏋️'].value_counts().idxmin()\",\n",
       " \"df['What is your eye color? 👁️'].mode()[0]\",\n",
       " \"df['What\\\\'s your sexual orientation?'].mode()[0]\",\n",
       " \"df['What area of knowledge is closer to you?'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['How long is your hair? 💇🏻♀️💇🏽♂️'].value_counts().nsmallest(3).index.tolist()\",\n",
       " \"df['What is your civil status? 💍'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df['What is your hair color? 👩🦰👱🏽'].value_counts().nsmallest(4).index.tolist()\",\n",
       " \"df['Gross annual salary (in euros) 💸'].nlargest(4).tolist()\",\n",
       " \"df.nsmallest(3, 'Happiness scale')['Happiness scale'].tolist()\",\n",
       " \"df['What is your age? 👶🏻👵🏻'].nlargest(5).tolist()\",\n",
       " \"df['What is your skin tone?'].value_counts().nsmallest(6).index.tolist()\",\n",
       " \"df['trip_distance'].gt(30).any()\",\n",
       " \"df['total_amount'].gt(100).any()\",\n",
       " \"df['passenger_count'].gt(6).any()\",\n",
       " \"df['payment_type'].isin([1, 2]).all()\",\n",
       " \"df['fare_amount'].max()\",\n",
       " \"df['PULocationID'].nunique()\",\n",
       " \"df['tip_amount'].mean()\",\n",
       " \"df[df['airport_fee'].notnull()]['airport_fee'].count()\",\n",
       " \"df['payment_type'].mode()[0]\",\n",
       " \"df['VendorID'].value_counts().idxmax()\",\n",
       " \"df['DOLocationID'].mode()[0]\",\n",
       " \"df['tpep_pickup_datetime'].min().date()\",\n",
       " \"df['PULocationID'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['RatecodeID'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['store_and_fwd_flag'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['payment_type'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df.nlargest(4, 'tolls_amount')['tolls_amount'].tolist()\",\n",
       " \"df.nlargest(3, 'trip_distance')['trip_distance'].tolist()\",\n",
       " \"df.nlargest(5, 'total_amount')['total_amount'].tolist()\",\n",
       " \"df.nlargest(6, 'fare_amount')['fare_amount'].tolist()\",\n",
       " \"df['borough'].eq('Brooklyn').any()\",\n",
       " \"df['descriptor'].str.contains('Dog').any()\",\n",
       " \"df['month_name'].eq('April').any()\",\n",
       " 'df[\\'agency\\'].isin([\"Mayor\\'s office of special enforcement\"]).any()',\n",
       " \"df[df['borough'] == 'Queens'].shape[0]\",\n",
       " \"df['agency'].nunique()\",\n",
       " \"df[df['hour'] == 0].shape[0]\",\n",
       " \"df['descriptor'].nunique()\",\n",
       " \"df['borough'].value_counts().idxmax()\",\n",
       " \"df['month_name'].value_counts().idxmax()\",\n",
       " \"df['weekday_name'].value_counts().idxmin()\",\n",
       " \"df['agency'].value_counts().idxmin()\",\n",
       " \"df['complaint_type'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df['agency'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['descriptor'].value_counts().nsmallest(2).index.tolist()\",\n",
       " \"df['weekday_name'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['hour'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['unique_key'].nunique() - df['unique_key'].nsmallest(3).tolist()\",\n",
       " \"df.groupby('hour')['unique_key'].count().nsmallest(2).index.tolist()\",\n",
       " \"df['complaint_type'].value_counts().nlargest(6).index.tolist()\",\n",
       " \"df['neighbourhood'].nunique() == 1\",\n",
       " \"df['host_identity_verified'].all()\",\n",
       " \"df['reviews_per_month'].gt(5).all()\",\n",
       " \"df['room_type'].isnull().any()\",\n",
       " \"df['neighbourhood'].nunique()\",\n",
       " \"df['price'].isnull().sum()\",\n",
       " \"df[df['review_scores_communication'] == 0].shape[0]\",\n",
       " \"df['bedrooms'].max()\",\n",
       " \"df['neighbourhood'].value_counts().idxmax()\",\n",
       " \"df['room_type'].mode()[0]\",\n",
       " \"df['property_type'].mode()[0]\",\n",
       " \"df['host_verifications'].value_counts().idxmin()\",\n",
       " \"df['neighbourhood_cleansed'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['property_type'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['host_verifications'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['room_type'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['review_scores_location'].nlargest(3).tolist()\",\n",
       " \"df['bedrooms'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df.groupby('host_id')['calculated_host_listings_count_entire_homes'].sum().nlargest(5).index.tolist()\",\n",
       " \"df.nsmallest(6, 'review_scores_communication')['review_scores_communication'].tolist()\",\n",
       " \"df['Overall'] > df['Potential']).any()\",\n",
       " \"df['Joined'].apply(lambda x: x.year) < (df['Age'] + 2005).mean() > 0\",\n",
       " \"df[(df['Preferred Foot'] == 'Left') & (df['Nationality'].str.startswith('B'))].any().any()\",\n",
       " \"df[(df['Height_ft'] > 6) & (df['Agility'] > 90)].any()\",\n",
       " \"df[df['Nationality<gx:category>'] == 'France']['Overall<gx:number>'].mean()\",\n",
       " \"df['Club<gx:category>'].nunique()\",\n",
       " \"df['Value_€<gx:currency>'].max()\",\n",
       " \"df[df['Position'] == 'ST'].shape[0]\",\n",
       " \"df['Nationality<gx:category>'].mode()[0]\",\n",
       " \"df['Preferred Foot<gx:category>'].mode()[0]\",\n",
       " \"df['Club<gx:category>'].value_counts().idxmax()\",\n",
       " \"df['Position<gx:category>'].mode()[0]\",\n",
       " \"df.groupby('Nationality<gx:category>')['Overall<gx:number>'].mean().nlargest(5).index.tolist()\",\n",
       " \"df.groupby('Club<gx:category>')['Value_€<gx:currency>'].sum().nlargest(3).index.tolist()\",\n",
       " \"df.groupby('Nationality<gx:category>')['Agility<gx:number>'].mean().nsmallest(4).index.tolist()\",\n",
       " \"df.groupby('Club<gx:category>')['Potential<gx:number>'].mean().nlargest(6).index.tolist()\",\n",
       " \"df['Overall'].nlargest(3).tolist()\",\n",
       " \"df.nsmallest(5, 'Potential')['Potential'].tolist()\",\n",
       " \"df['Value_€<gx:currency>'].nlargest(4).tolist()\",\n",
       " \"df['Wage_€'].nlargest(2).tolist()\",\n",
       " \"df['inj'].max() <= 500\",\n",
       " \"df['yr'] >= 2000\",\n",
       " \"df['len'].max() <= 100\",\n",
       " \"df['fat'].max() <= 100\",\n",
       " \"df['st'].nunique()\",\n",
       " \"df['mag'].max()\",\n",
       " \"df['len'].max()\",\n",
       " \"df['inj'].max()\",\n",
       " \"df['st'].value_counts().idxmax()\",\n",
       " \"df['mo'].mode()[0]\",\n",
       " \"df.loc[df['inj'] == df['inj'].max(), 'date'].iloc[0]\",\n",
       " \"df.loc[df['len'] == df['len'].max(), 'date'].iloc[0]\",\n",
       " \"df.groupby('st')['mag'].mean().nlargest(5).index.tolist()\",\n",
       " \"df.groupby('st')['inj'].sum().nlargest(2).index.tolist()\",\n",
       " \"df.groupby('st')['fat'].sum().nlargest(4).index.tolist()\",\n",
       " \"df.groupby('st')['len'].mean().nsmallest(2).index.tolist()\",\n",
       " \"df.nlargest(3, 'inj')['inj'].tolist()\",\n",
       " \"df['mag'].nlargest(5).tolist()\",\n",
       " \"df.nlargest(4, 'len')['len'].tolist()\",\n",
       " \"df.nlargest(6, 'fat')['fat'].tolist()\",\n",
       " \"df['PRCP'].gt(5).any() == False\",\n",
       " \"df[['TMIN', 'TMAX']].min().min() > 0\",\n",
       " \"df['SNWD'].max() <= 10\",\n",
       " \"df['TMAX'] >= 0).all()\",\n",
       " \"df['PRCP'].max()\",\n",
       " \"df['TMIN'].min()\",\n",
       " \"df['TMAX'].max()\",\n",
       " \"df['SNWD'].max()\",\n",
       " \"df.loc[df['PRCP'].idxmax(), 'DATE']\",\n",
       " \"df.loc[df['TMIN'] == df['TMIN'].min(), 'DATE'].iloc[0]\",\n",
       " \"df.loc[df['TMAX'] == df['TMAX'].max(), 'DATE'].iloc[0]\",\n",
       " \"df.loc[df['SNWD'] == df['SNWD'].max(), 'DATE'].iloc[0]\",\n",
       " \"df.nlargest(5, 'PRCP')['DATE'].tolist()\",\n",
       " \"df.nsmallest(3, 'TMIN')['DATE'].tolist()\",\n",
       " \"df.nlargest(4, 'TMAX')['DATE'].tolist()\",\n",
       " \"df.nlargest(2, 'SNWD')['DATE'].tolist()\",\n",
       " \"df.nlargest(3, 'PRCP')['PRCP'].tolist()\",\n",
       " \"df.nsmallest(5, 'TMIN')['TMIN'].tolist()\",\n",
       " \"df.nlargest(4, 'TMAX')['TMAX'].tolist()\",\n",
       " \"df.nlargest(2, 'SNWD')['SNWD'].tolist()\",\n",
       " \"df['Clothing ID'].nunique() > 20\",\n",
       " \"df['Age'].mean() > 50\",\n",
       " \"df['Department Name'].nunique() == 1\",\n",
       " \"df['Recommended IND'].all()\",\n",
       " \"df['Age'].mean()\",\n",
       " \"df['Positive Feedback Count'].max()\",\n",
       " \"df['Rating'].mode()[0]\",\n",
       " \"df['Clothing ID'].nunique()\",\n",
       " \"df['Department Name'].value_counts().idxmax()\",\n",
       " \"df['Class Name'].mode()[0]\",\n",
       " \"df['Division Name'].mode().iloc[0]\",\n",
       " \"df['Title'].value_counts().idxmax()\",\n",
       " \"df['Department Name'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['Class Name'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['Division Name'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['Rating'].nlargest(4).tolist()\",\n",
       " \"df['Age'].nlargest(5).tolist()\",\n",
       " \"df['Positive Feedback Count'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['Recommended IND'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['Clothing ID'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df.loc[df['Incident Year'] == 2023, 'Filed Online'].max() == 'True'\",\n",
       " \"df[df['Incident Day of Week'] == 'Monday']['Resolution'].nunique() == 1\",\n",
       " \"df[(df['Police District'] == 'Central') & (df['Supervisor District'] == 5)].any()\",\n",
       " \"df.duplicated(subset=['Latitude', 'Longitude'], keep=False).any()\",\n",
       " \"df['Incident Category'].nunique()\",\n",
       " \"df[df['Filed Online'] == 'Yes'].shape[0]\",\n",
       " \"df['Police District'].nunique()\",\n",
       " \"df.groupby('Incident Year')['Incident ID'].count().mean()\",\n",
       " \"df['Incident Category'].mode()[0]\",\n",
       " \"df['Incident Day of Week'].value_counts().idxmax()\",\n",
       " \"df[df['Filed Online'] == 'True']['Resolution'].mode()[0]\",\n",
       " \"df['Police District'].mode()[0]\",\n",
       " \"df['Incident Description'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['Police District'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df[df['Incident Day of Week'] == 'Friday']['Incident Category'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['Resolution'].value_counts().nlargest(6).index.tolist()\",\n",
       " \"df['Incident Year'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['Filed Online'].value_counts().loc[lambda x: x.index == 'Yes'].nlargest(3).index.tolist()\",\n",
       " \"df['Incident Year'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df.groupby('Incident Year')['Incident Category'].nunique().nlargest(6).index.tolist()\",\n",
       " \"df['ExerciseAngina'].all()\",\n",
       " \"df['RestingBP'].gt(200).any()\",\n",
       " \"df['HeartDisease'].nunique() < 2\",\n",
       " \"df['RestingECG'].value_counts().idxmax() == 'Normal'\",\n",
       " \"df['Age'].max()\",\n",
       " \"df['RestingBP'].min()\",\n",
       " \"df['Cholesterol'].mean()\",\n",
       " \"df['MaxHR'].std()\",\n",
       " \"df['ChestPainType'].mode()[0]\",\n",
       " \"df['RestingECG'].value_counts().idxmin()\",\n",
       " \"df.loc[df['HeartDisease'] == 1, 'ST_Slope'].mode().iloc[0]\",\n",
       " \"df[df['Sex'] == 'male']['ChestPainType'].value_counts().idxmin()\",\n",
       " \"df['ChestPainType'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['RestingECG'].value_counts().nsmallest(4).index.tolist()\",\n",
       " \"df[df['HeartDisease'] == 1]['ST_Slope'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df[df['Sex'] == 'male']['ChestPainType'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df.nlargest(5, 'Age')['Age'].tolist()\",\n",
       " \"df.nlargest(4, 'RestingBP')['RestingBP'].tolist()\",\n",
       " \"df.nsmallest(6, 'Cholesterol')['Cholesterol'].tolist()\",\n",
       " \"df['MaxHR'].nlargest(3).tolist()\",\n",
       " \"df['Status'].iloc[df['year_introduced'].idxmin()] == 'Operating'\",\n",
       " \"df['Speed'].astype(float).max() > 100\",\n",
       " \"df['Designer'].eq('Werner Stengel').all()\",\n",
       " \"df['G-force'].notnull().all()\",\n",
       " \"df['speed_mph'].astype(float).max()\",\n",
       " \"df[df['year_introduced'] == 2000].shape[0]\",\n",
       " \"df['G-force'].mean()\",\n",
       " \"df[df['Designer'] == 'Edwin Madeupname'].shape[0]\",\n",
       " \"df.loc[df['Speed'].astype(float) == df['Speed'].astype(float).max(), 'Manufacturer'].iloc[0]\",\n",
       " \"df.loc[df['G-force'].astype(float) == df['G-force'].astype(float).max(), 'Status'].iloc[0]\",\n",
       " \"df.loc[df['year_introduced'].idxmin(), 'Type']\",\n",
       " \"df.loc[df['Inversions'].idxmax(), 'Location']\",\n",
       " \"df.nlargest(3, 'speed1_value')['coaster_name'].tolist()\",\n",
       " \"df.nlargest(2, 'Inversions')['coaster_name'].tolist()\",\n",
       " \"df.nlargest(5, 'G-force')['Location'].tolist()\",\n",
       " \"df.nsmallest(4, 'year_introduced')['coaster_name'].tolist()\",\n",
       " \"df.nlargest(3, 'speed_mph')['speed_mph'].tolist()\",\n",
       " \"df.nlargest(2, 'G-force')['G-force'].tolist()\",\n",
       " \"df.nlargest(4, 'Height')['height_ft'].tolist()\",\n",
       " \"df.nsmallest(6, 'year_introduced')['year_introduced'].tolist()\",\n",
       " \"df['bedrooms'] == 5).any()\",\n",
       " \"df[(df['host_is_superhost'] == 't') & (df['instant_bookable'] == 't')].any()\",\n",
       " \"df['accommodates'].gt(10).any()\",\n",
       " \"df['review_scores_rating'].eq(10).any()\",\n",
       " 'df.shape[0]',\n",
       " \"df['bedrooms'].max()\",\n",
       " \"df['price'].astype(float).max()\",\n",
       " \"df['number_of_reviews'].max()\",\n",
       " \"df.loc[df['bedrooms'].idxmax(), 'host_response_time']\",\n",
       " \"df.loc[df['price'] == df['price'].max(), 'room_type'].iloc[0]\",\n",
       " \"df.loc[df['number_of_reviews'].idxmax(), 'property_type']\",\n",
       " \"df.loc[df['accommodates'].idxmax(), 'host_acceptance_rate']\",\n",
       " \"df[df['host_response_rate'] > '0%']['host_response_rate'].nsmallest(2).tolist()\",\n",
       " \"df.nsmallest(2, 'host_response_rate')['host_response_rate'].tolist()\",\n",
       " \"df['host_acceptance_rate'].nlargest(4).tolist()\",\n",
       " \"df['source'].value_counts().nsmallest(2).index.tolist()\",\n",
       " \"df.nlargest(3, 'price')['price'].tolist()\",\n",
       " \"df.nsmallest(2, 'price')['price'].tolist()\",\n",
       " \"df.nlargest(2, 'number_of_reviews')['number_of_reviews'].tolist()\",\n",
       " \"df.nlargest(5, 'accommodates')['accommodates'].tolist()\",\n",
       " \"df['GROUP'].str.contains('Fruits').any()\",\n",
       " \"df['SUB GROUP'].str.contains('Nuts').any()\",\n",
       " \"df['SCIENTIFIC NAME'].eq('Tilia argentea').any()\",\n",
       " \"df['FOOD NAME'].isin(['Angelica']).any()\",\n",
       " \"df['FOOD NAME'].nunique()\",\n",
       " \"df['GROUP'].nunique()\",\n",
       " \"df['SUB GROUP'].nunique()\",\n",
       " \"df['FOOD NAME'].nunique()\",\n",
       " \"df.loc[df['FOOD NAME'] == 'Kiwi', 'GROUP'].iloc[0]\",\n",
       " \"df.loc[df['SCIENTIFIC NAME'] == 'Tilia argentea', 'SUB GROUP'].iloc[0]\",\n",
       " \"df.loc[df['FOOD NAME'] == 'Colorado pinyon', 'SCIENTIFIC NAME'].iloc[0]\",\n",
       " \"df.loc[df['SCIENTIFIC NAME'] == 'Tilia argentea', 'FOOD NAME'].iloc[0]\",\n",
       " \"df['GROUP'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['GROUP'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['GROUP'].value_counts().nsmallest(5).index.tolist()\",\n",
       " \"df['SUB GROUP'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df['SUB GROUP'].value_counts().head(5).index.str.len().tolist()\",\n",
       " \"df['GROUP'].value_counts().nsmallest(2).tolist()\",\n",
       " \"df['GROUP'].value_counts().nlargest(3).tolist()\",\n",
       " \"df['GROUP'].str.len().value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['Occupation'].eq('Large Business').any()\",\n",
       " \"df['ProductPitched'].eq('King').any()\",\n",
       " \"df['Designation'].eq('VP').any()\",\n",
       " \"df['MaritalStatus'].str.contains('Unmarried').any()\",\n",
       " \"df['CustomerID'].nunique()\",\n",
       " \"df['Occupation'].nunique()\",\n",
       " \"df['Designation'].nunique()\",\n",
       " \"df['MaritalStatus'].nunique()\",\n",
       " \"df.loc[df['CustomerID'] == 200000, 'Occupation'].iloc[0]\",\n",
       " \"df.loc[df['CustomerID'] == 200001, 'ProductPitched'].iloc[0]\",\n",
       " \"df.loc[df['CustomerID'] == 200002, 'Designation'].iloc[0]\",\n",
       " \"df.loc[df['CustomerID'] == 200003, 'MaritalStatus'].iloc[0]\",\n",
       " \"df['Occupation'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['ProductPitched'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['Occupation'].value_counts().nsmallest(4).index.tolist()\",\n",
       " \"df['ProductPitched'].value_counts().nsmallest(2).index.tolist()\",\n",
       " \"df.nlargest(3, 'Age')['Age'].tolist()\",\n",
       " \"df.nsmallest(4, 'Age')['Age'].tolist()\",\n",
       " \"df.nlargest(2, 'MonthlyIncome')['MonthlyIncome'].tolist()\",\n",
       " \"df['DurationOfPitch'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['weekday_name'].isin(['Saturday', 'Sunday']).any()\",\n",
       " \"df['title'].str.len().gt(100).any()\",\n",
       " \"df['title_gx_cardiff_nlp_sentiment'].str.contains('negative').any()\",\n",
       " \"df['Clusters II'].str.contains('linux').any()\",\n",
       " \"df[df['partofday'] == 'morning'].shape[0]\",\n",
       " \"df['score'].max()\",\n",
       " \"df['descendants'].mean()\",\n",
       " \"df[df['season'] == 'Autumn'].shape[0]\",\n",
       " \"df['weekday_name'].value_counts().idxmax()\",\n",
       " \"df['title_gx_lang'].mode().iloc[0]\",\n",
       " \"df.loc[df['score'].idxmax(), 'season']\",\n",
       " \"df['partofday'].mode()[0]\",\n",
       " \"df['Clusters II'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['month_name'].value_counts().nsmallest(2).index.tolist()\",\n",
       " \"df['weekday_name'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df.groupby('season')['score'].count().nsmallest(4).index.tolist()\",\n",
       " \"df['score'].nlargest(3).tolist()\",\n",
       " \"df.nsmallest(5, 'title_gx_text_length')['title_gx_text_length'].tolist()\",\n",
       " \"df.nlargest(4, 'descendants')['descendants'].tolist()\",\n",
       " \"df.nsmallest(6, 'score')['score'].tolist()\",\n",
       " \"df['Number of Projects'].gt(7).any()\",\n",
       " \"df['Average Monthly Hours'] > 300).any()\",\n",
       " \"df['Satisfaction Level'].gt(0.5).all()\",\n",
       " \"df['Date Hired'].dt.year.eq(2019).any()\",\n",
       " \"df['Department'].nunique()\",\n",
       " \"df['Years in the Company'].max()\",\n",
       " \"df[df['Promoted in the last 5 years?'] == 'Yes'].shape[0]\",\n",
       " \"df['Average Monthly Hours'].mean()\",\n",
       " \"df['Department'].value_counts().idxmax()\",\n",
       " \"df['salary'].mode().iloc[0]\",\n",
       " \"df['Date Hired'].dt.year.value_counts().idxmax()\",\n",
       " \"df.groupby('salary')['Work Accident'].value_counts().unstack().idxmin().iloc[0]\",\n",
       " \"df['Department'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df[df['Promoted in the last 5 years?'] == 'Yes'].groupby('Department')['Promoted in the last 5 years?'].count().nlargest(3).index.tolist()\",\n",
       " \"df.groupby('Department')['Satisfaction Level'].mean().nsmallest(3).index.tolist()\",\n",
       " \"df.groupby('Department')['Average Monthly Hours'].mean().nsmallest(2).index.tolist()\",\n",
       " \"df['Date Hired'].dt.year.value_counts().nlargest(3).index.tolist()\",\n",
       " \"df[df['Left'] == 'Yes'].nlargest(3, 'Satisfaction Level')['Satisfaction Level'].tolist()\",\n",
       " \"df[df['Promoted in the last 5 years?'] == 'Yes'].nsmallest(5, 'Average Monthly Hours')['Average Monthly Hours'].tolist()\",\n",
       " \"df['Last Evaluation'].nlargest(6).tolist()\",\n",
       " \"df['Aircaft_Damage_Type'] == 'Total Loss'\",\n",
       " \"df['Incident_Cause(es)'].str.contains('undercarriage', case=False).any()\",\n",
       " \"df['Ground_Casualties'].astype(int).gt(0).any()\",\n",
       " \"df['Incident_Category'].str.contains('Collision').any()\",\n",
       " \"df['Aircaft_Model'].nunique()\",\n",
       " \"df['Onboard_Total'].max()\",\n",
       " \"df[df['Incident_Date'].str.startswith('2022-01')].shape[0]\",\n",
       " \"df[df['Fatalities'] > 0].shape[0]\",\n",
       " \"df['Aircaft_Model'].mode()[0]\",\n",
       " \"df.loc[df['Fatalities'] == df['Fatalities'].max(), 'Incident_Cause(es)'].iloc[0]\",\n",
       " \"df['Aircraft_Phase'].mode()[0]\",\n",
       " \"df.loc[df['Onboard_Total'].idxmax(), 'Incident_Location']\",\n",
       " \"df['Incident_Cause(es)'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['Incident_Location'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df['Aircaft_Operator'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['Aircaft_Damage_Type'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['Onboard_Passengers'].astype(int).nlargest(5).tolist()\",\n",
       " \"df['Onboard_Crew'].astype(int).nlargest(3).tolist()\",\n",
       " \"df['Onboard_Total'].nlargest(4).tolist()\",\n",
       " \"df['Ground_Casualties'].astype(int).nlargest(6).tolist()\",\n",
       " \"df['Precio'] > 1000000).any()\",\n",
       " \"df['Habitaciones'] > 10).any()\",\n",
       " \"df['Baños'].eq(0).any()\",\n",
       " \"df['Actualización'].max() - df['Actualización'].min() > pd.Timedelta(days=100)\",\n",
       " \"df['Precio'].max()\",\n",
       " \"df['Id'].nunique()\",\n",
       " \"df['Duración'].max()\",\n",
       " \"df['Superficie'].max()\",\n",
       " \"df['Tipo'].mode()[0]\",\n",
       " \"df['Anunciante'].value_counts().idxmax()\",\n",
       " \"df.loc[df['Precio'].idxmax(), 'Referencia']\",\n",
       " \"df.loc[df['Superficie'].idxmax(), 'Referencia']\",\n",
       " \"df['Tipo'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df['Tipo'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['Localidad'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['Distrito'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df.nlargest(5, 'Precio')['Precio'].tolist()\",\n",
       " \"df.nlargest(3, 'Duración')['Duración'].tolist()\",\n",
       " \"df.nlargest(4, 'Superficie')['Superficie'].tolist()\",\n",
       " \"df['Habitaciones'].nlargest(6).tolist()\",\n",
       " \"df['MonthlyCharges'] > 80).sum() > 2000\",\n",
       " \"df['PhoneService'].nunique() == 1\",\n",
       " \"df['InternetService'].eq('No').any()\",\n",
       " \"df[(df['SeniorCitizen'] == 1) & (df['Dependents'] == 'Yes')].any()\",\n",
       " \"df['customerID'].nunique()\",\n",
       " \"df['MonthlyCharges'].max()\",\n",
       " \"df['customerID'].nunique()\",\n",
       " \"df['tenure'].max()\",\n",
       " \"df['PaymentMethod'].mode()[0]\",\n",
       " \"df['Contract'].mode()[0]\",\n",
       " \"df.loc[df['TotalCharges'] == df['TotalCharges'].max(), 'customerID'].iloc[0]\",\n",
       " \"df.loc[df['MonthlyCharges'].idxmax(), 'customerID']\",\n",
       " \"df['InternetService'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['PaymentMethod'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['Contract'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['MultipleLines'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df.nlargest(5, 'TotalCharges')['TotalCharges'].tolist()\",\n",
       " \"df.nlargest(4, 'MonthlyCharges')['MonthlyCharges'].tolist()\",\n",
       " \"df.nlargest(6, 'tenure')['tenure'].tolist()\",\n",
       " \"df.nsmallest(3, 'tenure')['tenure'].tolist()\",\n",
       " \"df['review_scores_rating'].eq(100).any()\",\n",
       " \"df['host_total_listings_count'].gt(10).any()\",\n",
       " \"df['instant_bookable'].all()\",\n",
       " \"df['minimum_nights'].eq(365).any()\",\n",
       " \"df['host_id'].nunique()\",\n",
       " \"df['host_total_listings_count'].max()\",\n",
       " \"df['host_location'].nunique()\",\n",
       " \"df['review_scores_rating'].mean()\",\n",
       " \"df['host_location'].mode()[0]\",\n",
       " \"df.loc[df['bedrooms'] == df['bedrooms'].max(), 'name'].iloc[0]\",\n",
       " \"df['neighbourhood'].value_counts().idxmax()\",\n",
       " \"df['property_type'].mode()[0]\",\n",
       " \"df['host_location'].value_counts().nlargest(6).index.tolist()\",\n",
       " \"df['host_location'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['property_type'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['property_type'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df.nlargest(3, 'review_scores_rating')['review_scores_rating'].tolist()\",\n",
       " \"df.nsmallest(5, 'minimum_nights')['minimum_nights'].tolist()\",\n",
       " \"df['bedrooms'].unique().tolist()[:4]\",\n",
       " \"df.nsmallest(6, 'review_scores_location')['review_scores_location'].tolist()\",\n",
       " \"df[(df['tmin'] < 0) & (df['prec'] == 0)].shape[0] > 0\",\n",
       " \"df[(df['sol'] > 10) & (df['tmax'] < 20)].shape[0] > 0\",\n",
       " \"df[(df['velmedia'] < 5) & (df['racha'] > 15)].shape[0] > 0\",\n",
       " \"df[(df['season'] == 'summer') & (df['tmin'] < 10)].any()\",\n",
       " \"df[df['tmax'] > 30].shape[0]\",\n",
       " \"df[df['season'] == 'Winter']['tmin'].mean()\",\n",
       " \"df['fecha'].nunique()\",\n",
       " \"df['velmedia'].max()\",\n",
       " \"df.loc[df['tmax'] == df['tmax'].max(), 'weekday_name'].iloc[0]\",\n",
       " \"df.groupby('season')['sol'].mean().idxmax()\",\n",
       " \"df.groupby('month_name')['velmedia'].mean().idxmin()\",\n",
       " \"df.loc[df['presMax'] == df['presMax'].max(), 'fecha'].iloc[0]\",\n",
       " \"df.groupby('month_name')['tmax'].mean().nlargest(3).index.tolist()\",\n",
       " \"df.groupby('weekday_name')['prec'].sum().nlargest(5).index.tolist()\",\n",
       " \"df.groupby('season')['sol'].mean().nlargest(4).index.tolist()\",\n",
       " \"df.groupby('month_name')['tmin'].mean().nsmallest(2).index.tolist()\",\n",
       " \"df.nlargest(5, 'tmax')['tmax'].tolist()\",\n",
       " \"df.nsmallest(4, 'velmedia')['velmedia'].tolist()\",\n",
       " \"df.nlargest(3, 'sol')['sol'].tolist()\",\n",
       " \"df.groupby('dayofyear')['tmed'].mean().nlargest(6).index.tolist()\",\n",
       " \"df['RETRINOIN_xRZI'].gt(10000).any()\",\n",
       " \"df[(df['SEXO'] == 'female') & (df['NUTS1'] == 'ESTE')].any()\",\n",
       " \"df[(df['CONTROL'] == 'PRIVADO') & (df['MERCADO'] == 'NACIONAL')].any()\",\n",
       " \"df['RETRINOIN_WwQk'].lt(5000).any()\",\n",
       " \"df['NUTS1'].nunique()\",\n",
       " \"df[df['SEXO'] == 'male']['RETRINOIN'].mean()\",\n",
       " \"df['RETRINOIN_ac1q'].max()\",\n",
       " \"df['umap_cluster'].nunique()\",\n",
       " \"df['ANOS2'].value_counts().idxmax()\",\n",
       " \"df.groupby('NUTS1')['RETRINOIN'].mean().idxmax()\",\n",
       " \"df['MERCADO'].value_counts().idxmin()\",\n",
       " \"df['umap_cluster'].mode()[0]\",\n",
       " \"df['ANOS2'].value_counts().nsmallest(2).index.tolist()\",\n",
       " \"df['ANOS2'].value_counts().nsmallest(3).index.tolist()\",\n",
       " \"df['NUTS1'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df['umap_cluster'].value_counts().nsmallest(2).index.tolist()\",\n",
       " \"df['RETRINOIN'].nlargest(5).tolist()\",\n",
       " \"df.nsmallest(4, 'x')['x'].tolist()\",\n",
       " \"df.nlargest(3, 'y')['y'].tolist()\",\n",
       " \"df.nlargest(2, 'y')['y'].tolist()\",\n",
       " \"df['URLs'].iloc[df['Ranking'].idxmax()] == 'no code data science'\",\n",
       " \"df['Competition'].str.contains('Low').any()\",\n",
       " \"df['Avg. monthly searches'] > 1000).any()\",\n",
       " \"df['Ranking'].min() == df.loc[df['Ranking'] == df['Ranking'].min(), 'Avg. monthly searches'].max()\",\n",
       " \"df['URLs'].nunique()\",\n",
       " \"df['Ranking'].min()\",\n",
       " \"df['Avg. monthly searches'].min()\",\n",
       " \"df['Keyword'].nunique()\",\n",
       " \"df.loc[df['Ranking'].idxmax(), 'Competition']\",\n",
       " \"df.loc[df['Avg. monthly searches'].idxmax(), 'Keyword']\",\n",
       " \"df.loc[df['Ranking'] == df['Ranking'].max(), 'Competition'].iloc[0]\",\n",
       " \"df.loc[df['Ranking'].idxmin(), 'Keyword']\",\n",
       " \"df.nlargest(3, 'Avg. monthly searches')['URLs'].tolist()\",\n",
       " \"df[df['Ranking'] > 5]['Competition'].nsmallest(2).tolist()\",\n",
       " \"df.nsmallest(4, 'Ranking')['Keyword'].tolist()\",\n",
       " \"df.nsmallest(3, 'Ranking')['URLs'].tolist()\",\n",
       " \"df[df['Keyword'] == 'no code data science'].nsmallest(4, 'Ranking')['Ranking'].tolist()\",\n",
       " \"df[df['Competition'] == 'medium'].nsmallest(3, 'Avg. monthly searches')['Avg. monthly searches'].tolist()\",\n",
       " \"df[df['Competition'] == 'low'].nsmallest(5, 'Ranking')['Ranking'].tolist()\",\n",
       " \"df.nsmallest(2, 'Avg. monthly searches')['Avg. monthly searches'].tolist()\",\n",
       " \"df['quality'].eq(10).any()\",\n",
       " \"df['residual sugar'] > 15).any()\",\n",
       " \"df['alcohol'].idxmax() == df['quality'].idxmax()\",\n",
       " \"df['pH'] < 2.5).any()\",\n",
       " \"df['quality'].nunique()\",\n",
       " \"df['fixed acidity'].max()\",\n",
       " \"df['volatile acidity'].min()\",\n",
       " \"df[df['free sulfur dioxide'] > 50].shape[0]\",\n",
       " \"df.loc[df['alcohol'] == df['alcohol'].max(), 'quality'].iloc[0]\",\n",
       " \"df.loc[df['fixed acidity'] == df['fixed acidity'].max(), 'quality'].iloc[0]\",\n",
       " \"df.loc[df['volatile acidity'] == df['volatile acidity'].min(), 'quality'].iloc[0]\",\n",
       " \"df.loc[df['pH'] == df['pH'].max(), 'quality'].iloc[0]\",\n",
       " \"df.nlargest(3, 'alcohol')['quality'].tolist()\",\n",
       " \"df.nsmallest(2, 'residual sugar')['quality'].tolist()\",\n",
       " \"df.nlargest(5, 'density')['quality'].tolist()\",\n",
       " \"df.nsmallest(4, 'pH')['quality'].tolist()\",\n",
       " \"df.nlargest(4, 'quality')['alcohol'].tolist()\",\n",
       " \"df.nsmallest(3, 'quality')['volatile acidity'].tolist()\",\n",
       " \"df.nlargest(5, 'quality')['fixed acidity'].tolist()\",\n",
       " \"df.nsmallest(2, 'quality')['residual sugar']\",\n",
       " \"df['Total'].gt(1000).any()\",\n",
       " \"df['Payment'].str.contains('Cash').any()\",\n",
       " \"df.loc[df['Total'].idxmax(), 'Customer type'] == 'Member'\",\n",
       " \"df[(df['Rating'] > 9) & (df['Payment'] == 'Ewallet')].any()\",\n",
       " \"df['Branch'].nunique()\",\n",
       " \"df['Quantity'].max()\",\n",
       " \"df['Total'].min()\",\n",
       " \"df[df['City'] == 'Yangon'].shape[0]\",\n",
       " \"df.loc[df['Total'].idxmax(), 'Payment']\",\n",
       " \"df.loc[df['Total'].idxmax(), 'Product line']\",\n",
       " \"df.loc[df['Total'].idxmin(), 'Customer type']\",\n",
       " \"df.loc[df['Total'].idxmax(), 'Gender']\",\n",
       " \"df.nlargest(3, 'Total')['Payment'].tolist()\",\n",
       " \"df.nsmallest(2, 'Total')['Product line'].tolist()\",\n",
       " \"df.nlargest(5, 'Total')['Customer type'].tolist()\",\n",
       " \"df.nsmallest(4, 'Total')['Gender'].tolist()\",\n",
       " \"df.nlargest(4, 'Total')['Quantity'].tolist()\",\n",
       " \"df.nsmallest(3, 'Total')['Unit price'].tolist()\",\n",
       " \"df.nlargest(5, 'Total')['Rating'].tolist()\",\n",
       " \"df.nsmallest(2, 'Total')['gross income'].tolist()\",\n",
       " \"df['Age'] > 60).any()\",\n",
       " \"df['DiabetesPedigreeFunction'] > 2.5).any()\",\n",
       " \"df.loc[df['Glucose'] == df['Glucose'].max(), 'Outcome'].iloc[0] == 1\",\n",
       " \"df[(df['Pregnancies'] == 0) & (df['Outcome'] == 1)].any()\",\n",
       " \"df['Pregnancies'].max()\",\n",
       " \"df['BloodPressure'].min()\",\n",
       " \"df['BMI'].mean()\",\n",
       " \"df[df['Insulin'] > 150].shape[0]\",\n",
       " \"df.loc[df['BMI'].idxmax(), 'Outcome']\",\n",
       " \"df.loc[df['BloodPressure'] == df['BloodPressure'].min(), 'Outcome'].iloc[0]\",\n",
       " \"df.loc[df['Insulin'] == df['Insulin'].max(), 'Outcome'].iloc[0]\",\n",
       " \"df.loc[df['Glucose'] == df['Glucose'].min(), 'Outcome'].iloc[0]\",\n",
       " \"df.nlargest(3, 'Pregnancies')['Outcome'].tolist()\",\n",
       " \"df.nsmallest(2, 'BMI')['Outcome'].tolist()\",\n",
       " \"df.nlargest(5, 'Insulin')['Outcome'].tolist()\",\n",
       " \"df.nsmallest(4, 'BloodPressure')['Outcome'].tolist()\",\n",
       " \"df.nlargest(4, 'Pregnancies')['Age'].tolist()\",\n",
       " \"df.nsmallest(3, 'Glucose')['BMI'].tolist()\",\n",
       " \"df.nlargest(5, 'DiabetesPedigreeFunction')['BloodPressure'].tolist()\",\n",
       " \"df.nlargest(2, 'Insulin')['Glucose'].tolist()\",\n",
       " \"df['material_type'].eq('Op-Ed').any()\",\n",
       " \"df.loc[df['headline'].str.len() == df['headline'].str.len().max(), 'keywords'].str.contains('United States Politics and Government').any()\",\n",
       " \"df['date'].eq('2021-01-05').any()\",\n",
       " \"df['keywords'].str.split(',').apply(len).gt(10).any()\",\n",
       " \"df['material_type'].nunique()\",\n",
       " \"df['headline'].str.len().max()\",\n",
       " \"df[df['date'] == '2021-01-02'].shape[0]\",\n",
       " \"df['keywords'].str.split(',').str.len().max()\",\n",
       " \"df.loc[df['headline'].str.len() == df['headline'].str.len().max(), 'material_type'].iloc[0]\",\n",
       " \"df.loc[df['headline'].str.len().idxmin(), 'material_type']\",\n",
       " \"df.loc[df['keywords'].str.split().str.len().idxmax(), 'material_type']\",\n",
       " \"df.loc[df['keywords'].str.split().str.len().idxmin(), 'material_type']\",\n",
       " \"df.nlargest(3, 'headline', 'length')['material_type'].tolist()\",\n",
       " \"df.nsmallest(2, 'headline')['material_type'].tolist()\",\n",
       " \"df.nlargest(5, 'keywords', 'count')['material_type'].tolist()\",\n",
       " \"df.nsmallest(4, 'keywords')['material_type'].tolist()\",\n",
       " \"df.nlargest(4, 'keywords', 'count')['headline'].str.len().tolist()\",\n",
       " \"df.nsmallest(3, 'headline')['keywords'].str.split().str.len().tolist()\",\n",
       " \"df.nlargest(5, 'headline', key=lambda x: x.str.len())['headline'].str.len().tolist()\",\n",
       " \"df.nsmallest(2, 'keywords')['keywords'].str.split(',').str.len().tolist()\",\n",
       " \"df['Geographies'].value_counts().idxmax() == 'USA'\",\n",
       " 'df[(df[\\'What is the highest level of formal education that you have attained or plan to attain within the next 2 years?\\'] == \"Bachelor\\'s degree\") & (df[\\'In which country do you currently reside?\\'].str.contains(\"Africa\"))].any()',\n",
       " \"df['What programming language would you recommend an aspiring data scientist to learn first?'].value_counts().get('Python', 0) == len(df)\",\n",
       " \"df['Which of the following cloud computing platforms do you hope to become more familiar with in the next 2 years?'].str.contains('AWS').sum() > 1000\",\n",
       " \"df['Select the title most similar to your current role (or most recent title if retired)'].nunique()\",\n",
       " \"df['(Average) For how many years have you used machine learning methods?'].median()\",\n",
       " \"df[df['In which country do you currently reside?'] == 'United Kingdom'].shape[0]\",\n",
       " \"df['What programming languages do you use on a regular basis?'].value_counts().idxmax()\",\n",
       " \"df['What type of computing platform do you use most often for your data science projects?'].mode()[0]\",\n",
       " \"df['What programming languages do you use on a regular basis?'].mode()[0]\",\n",
       " \"df['In which country do you currently reside?'].value_counts().index[1]\",\n",
       " \"df['Select the title most similar to your current role (or most recent title if retired)'].mode().iloc[0]\",\n",
       " \"df['Geographies'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['General Segments'].value_counts().head(3).index.tolist()\",\n",
       " \"df['Select the title most similar to your current role (or most recent title if retired)'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['What programming languages do you use on a regular basis?'].value_counts().nlargest(6).index.tolist()\",\n",
       " \"df['What is your age (years)?'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df.groupby('(Average) For how many years have you used machine learning methods?')['(Average) For how many years have you used machine learning methods?'].mean().nlargest(3).index.tolist()\",\n",
       " \"df.nlargest(5, '(Average) What is your current yearly compensation (approximate $USD)?')['(Average) What is your current yearly compensation (approximate $USD)?'].tolist()\",\n",
       " \"df['(Average) What is the size of the company where you are employed?'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df[(df['country_code'] == 'GB') & (df['rating'] == 5)].shape[0] > df[(df['country_code'] == 'US') & (df['rating'] == 5)].shape[0]\",\n",
       " \"df[df['Company'] == 'Wise'].loc[df['country_code'] == 'GB', 'rating'].mean() > 4\",\n",
       " \"df['country_code'].eq('AO').any()\",\n",
       " \"df['rating'].eq(1).any()\",\n",
       " \"df[df['Company'] == 'Wise']['rating'].count()\",\n",
       " \"df[(df['Company'] == 'Wise') & (df['rating'] == 5)]['country_code'].nunique()\",\n",
       " \"df['rating'].max()\",\n",
       " \"df['rating'].mean()\",\n",
       " \"df.groupby('Company')['rating'].apply(lambda x: (x == 5).sum()).idxmax()\",\n",
       " \"df['Company'] == 'Wise').groupby('country_code')['rating'].count().idxmax()\",\n",
       " \"df['country_code'].value_counts().nsmallest(1).index.tolist()\",\n",
       " \"df.loc[df['rating'].idxmin(), 'Company']\",\n",
       " \"df[df['Company'] == 'Wise'][df['rating'] == 5]['country_code'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['Company'].unique().tolist()[:2]\",\n",
       " \"df['country_code'].value_counts().nsmallest(4).index.tolist()\",\n",
       " \"df['country_code'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['rating'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['rating'].value_counts().nsmallest(2).index.tolist()\",\n",
       " \"df[df['Company'] == 'Wise']['rating'].unique().tolist()[:4]\",\n",
       " \"df[df['Company'] == 'N26']['rating'].unique().tolist()[-5:]\",\n",
       " \"df['Education'].eq('PhD').any()\",\n",
       " \"df['Marital_Status'].str.contains('Married').any()\",\n",
       " \"df['Income'].gt(100000).any()\",\n",
       " \"df['NumWebPurchases'].gt(10).any()\",\n",
       " 'df.shape[0]',\n",
       " \"df['Income'].mean()\",\n",
       " \"df['NumWebPurchases'].max()\",\n",
       " \"df['Recency'].min()\",\n",
       " \"df['Education'].mode()[0]\",\n",
       " \"df['Marital_Status'].mode()[0]\",\n",
       " \"df.loc[df['Income'].idxmax(), 'ID']\",\n",
       " \"df.loc[df['Dt_Customer'] == df['Dt_Customer'].max(), 'ID'].iloc[0]\",\n",
       " \"df.nlargest(3, 'Income')['ID'].tolist()\",\n",
       " \"df['Education'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['Marital_Status'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df.nlargest(3, 'NumWebPurchases')['ID'].tolist()\",\n",
       " \"df['Income'].nlargest(3).tolist()\",\n",
       " \"df['Recency'].nlargest(2).tolist()\",\n",
       " \"df.nsmallest(3, 'Income')['Income'].tolist()\",\n",
       " \"df.nsmallest(2, 'Recency')['Recency'].tolist()\",\n",
       " \"df['salary'].str.contains('high').any()\",\n",
       " \"df['Work Accident'].nunique() > 0\",\n",
       " \"df['Average Monthly Hours'].gt(100).any()\",\n",
       " \"df['Satisfaction Level'].max() > 0.9\",\n",
       " 'df.shape[0]',\n",
       " \"df['Satisfaction Level'].median()\",\n",
       " \"df['Number of Projects'].max()\",\n",
       " \"df['Marital_Status'].mode()[0]\",\n",
       " \"df['salary'].mode().iloc[0]\",\n",
       " \"df['Marital_Status'].mode()[0]\",\n",
       " \"df[df['Department'] == 'sales']['salary'].mode().iloc[0]\",\n",
       " \"df[df['Work Accident'] == 'Yes'][df['Department'] == 'sales']['salary'].mode()[0]\",\n",
       " \"df['Satisfaction Level'].nlargest(3).tolist()\",\n",
       " \"df['Work Accident'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['Marital_Status'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['Years in the Company'].nlargest(3).tolist()\",\n",
       " \"df[df['salary'] == 'medium']['Department'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df[df['Department'] == 'sales'].nlargest(2, 'Satisfaction Level')['salary'].tolist()\",\n",
       " \"df.groupby('Department')['Average Monthly Hours'].mean().nlargest(3).tolist()\",\n",
       " \"df[df['Work Accident'] == 'No']['Satisfaction Level'].nsmallest(2).tolist()\",\n",
       " \"df[(df['Ladder score'] > 7.5) & (df['Generosity'] > 0.2) & (df['Social support'] > 0.7)].any()\",\n",
       " \"df[(df['Regional indicator'] == 'Western Europe') & (df['Perceptions of corruption'] > 0.5) & (df['Ladder score'] > 7) & (df['Social support'] > 0.7)].shape[0] > 0\",\n",
       " \"df[(df['Regional indicator'] == 'Western Europe') & (df['Perceptions of corruption'] > 0.5)].shape[0] > 0\",\n",
       " \"df['Social support'] < 0.5).any()\",\n",
       " \"df[(df['Regional indicator'] == 'Western Europe') & (df['Generosity'] > 0.2) & (df['Social support'] > 0.7)]['Ladder score'].mean()\",\n",
       " \"df[(df['Regional indicator'] == 'Sub-Saharan Africa') & (df['Ladder score'] < 5) & (df['Social support'] < 0.5)]['Perceptions of corruption'].mean()\",\n",
       " \"df['Generosity'].max()\",\n",
       " \"df['Social support'].min()\",\n",
       " \"df[df['Ladder score'] > 7][df['Generosity'] > 0.2].groupby('Regional indicator')['Country name'].nunique().idxmax()\",\n",
       " \"df[df['Generosity'] > 0.2]['Regional indicator'].value_counts().idxmax()\",\n",
       " \"df.loc[df['Perceptions of corruption'] < 0.5, 'Regional indicator'].mode()[0]\",\n",
       " \"df[df['Social support'] > 0.7]['Regional indicator'].mode().iloc[0]\",\n",
       " \"df[(df['Ladder score'] > 7) & (df['Generosity'] > 0.2)].groupby('Regional indicator')['Country name'].nunique().nlargest(3).index.tolist()\",\n",
       " \"df[df['Generosity'] > 0.2].groupby('Regional indicator')['Country name'].nunique().nlargest(3).index.tolist()\",\n",
       " \"df[(df['Regional indicator'] == 'Western Europe') & (df['Generosity'] > 0.2) & (df['Social support'] > 0.7)].nlargest(3, 'Ladder score')['Ladder score'].tolist()\",\n",
       " \"df[df['Regional indicator'] == 'Sub-Saharan Africa'].nlargest(3, 'Perceptions of corruption')['Perceptions of corruption'].tolist()\",\n",
       " \"df[df['Regional indicator'] == 'Western Europe'].nlargest(3, 'Generosity')['Generosity'].tolist()\",\n",
       " \"df[df['Regional indicator'] == 'Sub-Saharan Africa'].nlargest(3, 'Social support')['Social support'].tolist()\",\n",
       " \"df[df['Regional indicator'] == 'Western Europe'].nsmallest(3, 'Ladder score')['Country name'].tolist()\",\n",
       " \"df[df['Regional indicator'] == 'Sub-Saharan Africa'].nsmallest(3, 'Perceptions of corruption')['Perceptions of corruption'].tolist()\",\n",
       " \"df['Year'].loc[df['Rank'] == df['Rank'].min()].iloc[0] == 1965\",\n",
       " \"df.loc[df['Rank'].idxmin(), 'Lyrics'].str.contains('love')\",\n",
       " \"df['Artist'].iloc[df['Rank'].idxmax()] == df['Artist'].iloc[df['Rank'].idxmin()]\",\n",
       " \"df['Lyrics'].isnull().any()\",\n",
       " \"df[df['Year'] == 1965].shape[0]\",\n",
       " \"df.loc[df['Rank'] == df['Rank'].min(), 'Year'].iloc[0]\",\n",
       " \"df.loc[df['Lyrics'].str.len().idxmax(), 'Rank']\",\n",
       " \"df['Artist'].nunique()\",\n",
       " \"df.loc[df['Rank'] == df['Rank'].min(), 'Artist'].iloc[0]\",\n",
       " \"df.loc[df['Rank'] == df['Rank'].min(), 'Song'].iloc[0]\",\n",
       " \"df.loc[df['Lyrics'].str.lower().str.count('love').idxmax(), 'Song']\",\n",
       " \"df.loc[df['Year'] == df['Year'].min(), 'Song'].iloc[0]\",\n",
       " \"df.nlargest(5, 'Rank')['Artist'].tolist()\",\n",
       " \"df.nsmallest(3, 'Lyrics')['Song'].tolist()\",\n",
       " \"df.nlargest(4, 'Rank')[['Song']].loc[df['Year'] == df['Year'].max()]['Song'].tolist()\",\n",
       " \"df.nsmallest(5, 'Rank')['Artist'].tolist()\",\n",
       " \"df[df['Lyrics'].str.contains('love', case=False)].nlargest(3, 'Rank')['Rank'].tolist()\",\n",
       " \"df.nsmallest(4, 'Rank')['Year'].tolist()\",\n",
       " \"df.nsmallest(2, 'Year')['Rank'].tolist()\",\n",
       " \"df.nlargest(5, 'Lyrics')['Year'].tolist()\",\n",
       " \"df.loc[(df['Year'] == 1965) & (df['Artist'] == 'Beatles'), 'Rank'].min() == 1\",\n",
       " \"df.loc[df['Year'] == 1965].loc[df['Rank'] == df[df['Year'] == 1965]['Rank'].min(), 'Artist'].iloc[0]\",\n",
       " \"df[df['Year'] == 1965].nlargest(3, 'Rank')['Artist'].tolist()\",\n",
       " \"df[df['Artist'] == 'Beatles'][df['Lyrics'].str.contains('love')].nlargest(3, 'Rank')['Year'].tolist()\",\n",
       " \"df['count'].gt(20000).any()\",\n",
       " \"df['name_origin'].eq('Jefferson').any()\",\n",
       " \"df['name_dest'].eq('Baldwin').any()\",\n",
       " \"df['lat_dest'] > 60).any()\",\n",
       " \"df['name_dest'].nunique()\",\n",
       " \"df.groupby(['name_origin', 'name_dest'])['count'].mean()\",\n",
       " \"df['count'].max()\",\n",
       " \"df['name_origin'].nunique()\",\n",
       " \"df.loc[df['count'].idxmax(), 'name_origin']\",\n",
       " \"df.loc[df['count'].idxmax(), 'name_dest']\",\n",
       " \"df.loc[df['lat_origin'].idxmin(), 'name_origin']\",\n",
       " \"df.loc[df['lon_dest'].idxmax(), 'name_dest']\",\n",
       " \"df.groupby('name_origin')['count'].mean().nlargest(3).index.tolist()\",\n",
       " \"df.groupby('name_dest')['count'].mean().nsmallest(4).index.tolist()\",\n",
       " \"df.groupby('name_origin')['lat_origin'].mean().nlargest(5).index.tolist()\",\n",
       " \"df.groupby('name_origin').lon_origin.mean().nsmallest(2).index.tolist()\",\n",
       " \"df.nlargest(5, 'count')['count'].tolist()\",\n",
       " \"df.nlargest(3, 'lat_dest')['lat_dest'].tolist()\",\n",
       " \"df.nsmallest(4, 'lon_origin')['lon_origin'].tolist()\",\n",
       " \"df.groupby('dest')['count'].mean().nlargest(6).index.tolist()\",\n",
       " \"df['views'].gt(1000000).any()\",\n",
       " \"df['all_speakers'].str.contains('Elon Musk').any()\",\n",
       " \"df['event'].str.contains('TEDx').any()\",\n",
       " \"df['available_lang'].str.split(',').str.len().gt(10).any()\",\n",
       " \"df['all_speakers'].nunique()\",\n",
       " \"df['views'].mean()\",\n",
       " \"df['duration'].max()\",\n",
       " \"df[df['comments'] > 500].shape[0]\",\n",
       " \"df.groupby('event')['views'].mean().idxmax()\",\n",
       " \"df.loc[df['comments'] == df['comments'].max(), 'speaker_1'].iloc[0]\",\n",
       " \"df.loc[df['views'].idxmin(), 'title']\",\n",
       " \"df.loc[df['duration'] == df['duration'].max(), 'event'].iloc[0]\",\n",
       " \"df.groupby('event')['comments'].mean().nlargest(4).index.tolist()\",\n",
       " \"df['speaker_1'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df.groupby('event')['duration'].mean().nsmallest(5).index.tolist()\",\n",
       " \"df['event'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df.nlargest(4, 'views')['views'].tolist()\",\n",
       " \"df.nsmallest(3, 'duration')['duration'].tolist()\",\n",
       " \"df.nlargest(5, 'comments')['comments'].tolist()\",\n",
       " \"df.nsmallest(6, 'views')['views'].tolist()\",\n",
       " \"df.loc[df['age'] == df['age'].max(), 'stroke'].iloc[0] == 1\",\n",
       " \"df['hypertension'].loc[df['smoking_status'] == 'smokes'].nunique() == 1 and df['hypertension'].loc[df['smoking_status'] == 'smokes'].iloc[0] == 1\",\n",
       " \"df['heart_disease'].eq(1).all() == df['ever_married'].eq('Yes').loc[df['heart_disease'] == 1].all()\",\n",
       " \"df.groupby('gender')['avg_glucose_level'].mean().diff().iloc[-1] > 0\",\n",
       " \"df['stroke'].sum()\",\n",
       " \"df[df['smoking_status'] == 'smokes']['age'].mean()\",\n",
       " \"df['avg_glucose_level'].max()\",\n",
       " \"df['work_type'].nunique()\",\n",
       " \"df[df['stroke'] == 1]['work_type'].mode().iloc[0]\",\n",
       " \"df.loc[df['age'] == df['age'].min(), 'smoking_status'].iloc[0]\",\n",
       " \"df.loc[df['bmi'] == df['bmi'].max(), 'Residence_type'].iloc[0]\",\n",
       " \"df.loc[df['avg_glucose_level'] == df['avg_glucose_level'].min(), 'gender'].iloc[0]\",\n",
       " \"df[df['heart_disease'] == 1]['work_type'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df[df['stroke'] == 1]['smoking_status'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df[df['hypertension'] == 1]['Residence_type'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df[df['ever_married'] == 'No']['work_type'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df[df['stroke'] == 1]['age'].nlargest(5).tolist()\",\n",
       " \"df[df['smoking_status'] == 'smokes'].nsmallest(3, 'bmi')['bmi'].tolist()\",\n",
       " \"df[df['heart_disease'] == 1].nlargest(4, 'avg_glucose_level')['avg_glucose_level'].tolist()\",\n",
       " \"df[df['ever_married'] == 'No']['age'].nlargest(6).tolist()\",\n",
       " \"df['num_sentence'].gt(10).any()\",\n",
       " \"df[(df['country'] == 'USA') & (df['num_sentence'] > 5)].any()\",\n",
       " \"df['reflection_period'].eq('affection').any()\",\n",
       " \"df[(df['marital'] == 'married') & (df['reflection_period'] == 'exercise')].shape[0] > 0\",\n",
       " \"df['reflection_period'].nunique()\",\n",
       " \"df['num_sentence'].mean()\",\n",
       " \"df['age'].max()\",\n",
       " \"df[df['country'] == 'IND'].shape[0]\",\n",
       " \"df.groupby('country')['num_sentence'].mean().idxmax()\",\n",
       " \"df[df['ground_truth_category'] == 'affection']['gender'].value_counts().idxmax()\",\n",
       " \"df.loc[df['age'] == df['age'].max(), 'country'].iloc[0]\",\n",
       " \"df[df['ground_truth_category'] == 'bonding']['marital'].value_counts().idxmax()\",\n",
       " \"df.groupby('country')['num_sentence'].mean().nlargest(3).index.tolist()\",\n",
       " \"df['predicted_category'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df.groupby('country')['age'].mean().nsmallest(5).index.tolist()\",\n",
       " \"df.groupby('gender')['num_sentence'].sum().nlargest(2).index.tolist()\",\n",
       " \"df.nsmallest(4, 'num_sentence')['num_sentence'].tolist()\",\n",
       " \"df.nlargest(3, 'age')['age'].tolist()\",\n",
       " \"df.nlargest(5, 'num_sentence')['num_sentence'].tolist()\",\n",
       " \"df.nsmallest(6, 'age')['age'].tolist()\",\n",
       " \"df.loc[df['age'].idxmin(), 'met'] == 1\",\n",
       " \"df['samerace'].all()\",\n",
       " \"df['gender'].loc[df['expected_num_matches'] > 5].nunique() == 1 and df['gender'].unique()[0] == 'male'\",\n",
       " \"df['age'].mean() > df[df['match'] == 0]['age'].mean()\",\n",
       " \"df['match'].sum()\",\n",
       " \"df[df['samerace'] == 1]['age'].mean()\",\n",
       " \"df['expected_num_matches'].max()\",\n",
       " \"df['race'].nunique()\",\n",
       " \"df.loc[df['match'] == 1, 'race'].mode()[0]\",\n",
       " \"df.loc[df['age'].idxmin(), 'gender']\",\n",
       " \"df.loc[df['expected_num_matches'].idxmax(), 'race']\",\n",
       " \"df.loc[df['age'] == df['age'].min(), 'wave'].iloc[0]\",\n",
       " \"df[df['match'] == 1]['wave'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df[df['expected_num_matches'] > 5]['race'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df[df['samerace'] == 1]['wave'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df[df['expected_num_matches'] == 0]['gender'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df[df['match'] == 1]['age'].nlargest(5).tolist()\",\n",
       " \"df[df['match'] == 1]['expected_num_matches'].nsmallest(3).tolist()\",\n",
       " \"df[df['samerace'] == 1]['age'].nlargest(4).tolist()\",\n",
       " \"df[df['expected_num_matches'] == 0]['age'].nsmallest(6).tolist()\",\n",
       " \"df['retweet_count'].gt(10).any()\",\n",
       " \"df['airline_sentiment'].eq('negative').any() & df['airline'].eq('United').any()\",\n",
       " \"df['negativereason'].str.contains('Late Flight').any()\",\n",
       " \"df[(df['user_timezone'] == 'Eastern Time (US & Canada)') & (df['airline_sentiment'] == 'positive')].any()\",\n",
       " \"df['user_timezone'].nunique()\",\n",
       " \"df['airline_sentiment_confidence'].mean()\",\n",
       " \"df['retweet_count'].max()\",\n",
       " \"df[df['airline'] == 'Virgin America'].shape[0]\",\n",
       " \"df.groupby('airline')['airline_sentiment_confidence'].mean().idxmax()\",\n",
       " \"df[df['airline'] == 'American']['negativereason'].mode().iloc[0]\",\n",
       " \"df.loc[df['airline_sentiment_confidence'].idxmax(), 'user_timezone']\",\n",
       " \"df['airline'].value_counts()[df['airline_sentiment'] == 'negative'].idxmax()\",\n",
       " \"df.groupby('airline')['airline_sentiment_confidence'].mean().nlargest(3).index.tolist()\",\n",
       " \"df['negativereason'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['user_timezone'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['airline'].value_counts().nsmallest(2).index.tolist()\",\n",
       " \"df.nsmallest(4, 'airline_sentiment_confidence')['tweet_id'].tolist()\",\n",
       " \"df.nlargest(3, 'retweet_count')['retweet_count'].tolist()\",\n",
       " \"df.nsmallest(2, 'airline_sentiment_confidence')['airline_sentiment_confidence'].tolist()\",\n",
       " \"df.nlargest(6, 'airline_sentiment_confidence')['airline_sentiment_confidence'].tolist()\",\n",
       " \"df.loc[df['absences'].idxmax(), 'address'] == 'rural'\",\n",
       " \"df['age'].loc[df['romantic'] == 'yes'].gt(17).all()\",\n",
       " \"df['G3'][df['Dalc'] > 2].mean() < df['G3'][df['Dalc'] <= 2].mean()\",\n",
       " \"df[(df['Pstatus'] == 'T')]['freetime'].gt(3).all()\",\n",
       " \"df[df['Medu'] > 3].shape[0]\",\n",
       " \"df[df['higher'] == 'yes']['age'].mean()\",\n",
       " \"df['absences'].max()\",\n",
       " \"df['school'].nunique()\",\n",
       " \"df[df['higher'] == 'yes']['Mjob'].mode()[0]\",\n",
       " \"df.loc[df['G3'] == df['G3'].max(), 'sex'].iloc[0]\",\n",
       " \"df.loc[df['absences'].idxmax(), 'school']\",\n",
       " \"df.loc[df['G3'] == df['G3'].max(), 'famsize'].iloc[0]\",\n",
       " \"df[df['higher'] == 'yes']['reason'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df[df['G3'] > 10]['Mjob'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df[df['absences'] > 10]['school'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df[df['G3'] < 10]['famrel'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df[df['higher'] == 'yes']['age'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df[df['absences'] > 10].nsmallest(3, 'G3')['G3'].tolist()\",\n",
       " \"df[df['Medu'] > 3]['age'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df[df['higher'] == 'no'].nsmallest(6, 'G3')['G3'].tolist()\",\n",
       " \"df['ApprovalFY'].max() == df.loc[df['UrbanRural'] == 1, 'ApprovalFY'].max()\",\n",
       " \"df['UrbanRural'].loc[df['NoEmp'] > 10].all()\",\n",
       " \"df['Sector'].loc[df['default_amount'] > 250000].nunique() == 1 and df['Sector'].loc[df['default_amount'] > 250000].iloc[0] == 'Food'\",\n",
       " \"df['RetainedJob'].mean() > df[df['UrbanRural'] == 0]['RetainedJob'].mean()\",\n",
       " \"df[df['FranchiseCode'] == 1].shape[0]\",\n",
       " \"df[df['Sector'] == 'Retail']['DisbursementGross'].mean()\",\n",
       " \"df['ApprovalFY'].max()\",\n",
       " \"df['Sector'].nunique()\",\n",
       " \"df.loc[df['FranchiseCode'] == 1, 'Sector'].mode()[0]\",\n",
       " \"df.loc[df['DisbursementGross'] == df['DisbursementGross'].max(), 'State'].iloc[0]\",\n",
       " \"df.loc[df['default_amount'] == df['default_amount'].max(), 'Bank'].iloc[0]\",\n",
       " \"df.loc[df['NoEmp'] == df['NoEmp'].max(), 'Sector'].iloc[0]\",\n",
       " \"df[df['FranchiseCode'] == 1]['Sector'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df[df['ApprovalFY'] < 2000]['Bank'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df[df['DisbursementGross'] > 1000000]['State'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df[df['default_amount'] > 500000]['Sector'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df[df['Sector'] == 'Retail']['ApprovalFY'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df[df['FranchiseCode'] == 1].nlargest(3, 'DisbursementGross')['DisbursementGross'].tolist()\",\n",
       " \"df[df['State'] == 'CA']['ApprovalFY'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df[df['State'] == 'NY'].nlargest(6, 'default_amount')['default_amount'].tolist()\",\n",
       " \"df.loc[df['genre'] == 'Drama', 'year'].min() == df.loc[(df['genre'] == 'Drama') & (df['year'] == df['year'].min()), 'country'].iloc[0] == 'USA'\",\n",
       " \"df[(df['country'] == 'USA') & (df['duration'] > 80) & (df['genre'] == 'Drama')]['language'].nunique() == 1\",\n",
       " \"df[(df['votes'] > 90) & (df['country'] == 'USA')]['language'].nunique() == 1\",\n",
       " \"df.groupby(df['country'] == 'USA')['duration'].mean().iloc[0] > df.groupby(df['country'] != 'USA')['duration'].mean().iloc[0]\",\n",
       " \"df[(df['country'] == 'USA') & (df['genre'] == 'Drama') & (df['metascore'] == 100)].shape[0]\",\n",
       " \"df[(df['genre'] == 'Drama') & (df['country'] == 'USA') & (df['language'] == 'English')]['duration'].mean()\",\n",
       " \"df.loc[(df['language'] == 'English') & (df['country'] == 'USA'), 'year'].max()\",\n",
       " \"df[df['country'] == 'USA']['language'].nunique()\",\n",
       " \"df.loc[(df['language'] == 'English') & (df['country'] == 'USA') & (df['metascore'] == 100), 'genre'].mode()[0]\",\n",
       " \"df.loc[df['duration'].idxmax(), 'country']\",\n",
       " \"df.loc[df['country'] == 'USA'].loc[df['metascore'] == df['metascore'].max(), 'language'].iloc[0]\",\n",
       " \"df.loc[df['language'] == 'English'].loc[df['country'] == 'USA'].loc[df['votes'] == df['votes'].max(), 'genre'].iloc[0]\",\n",
       " \"df[(df['language'] == 'English') & (df['country'] == 'USA') & (df['metascore'] == 100)]['genre'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df[df['language'] == 'English'][df['year'] < 2000]['country'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df[df['country'] == 'USA'][df['duration'] > 180]['language'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df[(df['language'] == 'English') & (df['country'] == 'USA') & (df['metascore'] > 90)]['genre'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df[(df['language'] == 'English') & (df['country'] == 'USA') & (df['genre'] == 'Drama')]['year'].nlargest(5).tolist()\",\n",
       " \"df[(df['language'] == 'English') & (df['country'] == 'USA') & (df['metascore'] == 100)].nlargest(3, 'duration')['duration'].tolist()\",\n",
       " \"df[df['language'] == 'English'].groupby('year').filter(lambda x: x['country'].str.contains('USA').any()).count().nlargest(4).index.tolist()\",\n",
       " \"df[df['language'] == 'English'].nlargest(6, 'metascore')['metascore'].tolist()\",\n",
       " \"df['duration_ms'].min() > 300000 and df['popularity'].min() == df.loc[df['duration_ms'].idxmin(), 'popularity']\",\n",
       " \"df[(df['release_year'] == 2020) & (df['popularity'] == df['popularity'].max())].any()\",\n",
       " \"df['duration_ms'].idxmax() == df['energy'].idxmax()\",\n",
       " \"df['energy'].idxmax() == df['popularity'].idxmax()\",\n",
       " \"df['artists'].nunique()\",\n",
       " \"df['duration_ms'].mean()\",\n",
       " \"df['popularity'].max()\",\n",
       " \"df['release_year'].value_counts().idxmax()\",\n",
       " \"df.loc[df['popularity'] == df['popularity'].max(), 'artists'].iloc[0]\",\n",
       " \"df.loc[df['popularity'].idxmax(), 'release_month']\",\n",
       " \"df.loc[df['duration_ms'] == df['duration_ms'].max(), 'name'].iloc[0]\",\n",
       " \"df.loc[df['energy'].idxmax(), 'name']\",\n",
       " \"df.nsmallest(2, 'duration_ms')['name'].tolist()\",\n",
       " \"df.nlargest(3, 'popularity')['name'].tolist()\",\n",
       " \"df.nlargest(3, 'duration_ms')['artists'].tolist()\",\n",
       " \"df.nlargest(2, 'energy')['name'].tolist()\",\n",
       " \"df.nlargest(5, 'popularity')['popularity'].tolist()\",\n",
       " \"df.nlargest(3, 'duration_ms')['duration_ms'].tolist()\",\n",
       " \"df['release_year'].nlargest(4).tolist()\",\n",
       " \"df.nlargest(3, 'energy')['energy'].tolist()\",\n",
       " \"df.loc[df['ID'] == df['ID'].min(), 'Age'].iloc[0] > 30\",\n",
       " \"df[df['Team'] == df['Team'].value_counts().idxmax()][df['Medal'] == 'Gold'].groupby('ID').filter(lambda x: len(x) >= 2).any()\",\n",
       " \"df['Weight'].max() == df[df['Sport'] == df['Sport'].value_counts().idxmax()]['Weight'].max()\",\n",
       " \"df.loc[df['Height'] == df['Height'].max(), 'Medal'].notnull().any()\",\n",
       " \"df['Team'].nunique()\",\n",
       " \"df['Age'].mean()\",\n",
       " \"df['Weight'].max()\",\n",
       " \"df['Year'].value_counts().idxmax()\",\n",
       " \"df.loc[df['Weight'].idxmax(), 'Name']\",\n",
       " \"df.loc[df['Height'] == df['Height'].max(), 'City'].iloc[0]\",\n",
       " \"df.groupby('Name')['Games'].count().idxmax()\",\n",
       " \"df.loc[df['Medal'].notnull(), 'Sport'].value_counts().idxmax()\",\n",
       " \"df['Team'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df.nlargest(3, 'Weight')['Name'].tolist()\",\n",
       " \"df['City'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['Name'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df['Age'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df.nlargest(3, 'Weight')['Weight'].tolist()\",\n",
       " \"df['Year'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['Height'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['customer_age'].lt(18).any()\",\n",
       " \"df['total_trans_amt'].eq(0).any()\",\n",
       " \"df['credit_limit'].gt(50000).any()\",\n",
       " \"df['months_inactive_12_mon'].gt(12).any()\",\n",
       " \"df['credit_limit'].max()\",\n",
       " \"df['total_trans_amt'].max()\",\n",
       " \"df['total_revolving_bal'].max()\",\n",
       " \"df['customer_age'].max()\",\n",
       " \"df['education_level'].mode()[0]\",\n",
       " \"df['income_category'].mode().iloc[0]\",\n",
       " \"df['gender'].value_counts().idxmax()\",\n",
       " \"df['attrition_flag'].mode().iloc[0]\",\n",
       " \"df['education_level'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['income_category'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['income_category'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['income_category'].value_counts().nlargest(2).index.tolist()\",\n",
       " \"df.nlargest(5, 'customer_age')['customer_age'].tolist()\",\n",
       " \"df.nlargest(5, 'credit_limit')['id'].tolist()\",\n",
       " \"df.nlargest(6, 'total_trans_amt')['id'].tolist()\",\n",
       " \"df.nlargest(6, 'total_revolving_bal')['id'].tolist()\",\n",
       " \"df['Location'].str.contains('New York, NY').any()\",\n",
       " \"df['Founded'].lt(1900).any()\",\n",
       " \"df['python_yn'].any()\",\n",
       " \"df['Job Title'].str.contains('Data Engineer').any()\",\n",
       " \"df['Job Title'].nunique()\",\n",
       " \"df['Founded'].mean()\",\n",
       " \"df['Rating'].max()\",\n",
       " \"df[df['Type of ownership'] == 'Government'].shape[0]\",\n",
       " \"df.groupby('Job Title')['avg_salary'].mean().idxmax()\",\n",
       " \"df['job_state'].value_counts().idxmax()\",\n",
       " \"df.loc[df['max_salary'] == df['max_salary'].max(), 'Sector'].iloc[0]\",\n",
       " \"df['Size'].value_counts().idxmax()\",\n",
       " \"df['Sector'].value_counts().nlargest(3).index.tolist()\",\n",
       " \"df['Industry'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['job_state'].value_counts().nlargest(5).index.tolist()\",\n",
       " \"df['Size'].value_counts().nsmallest(3).index.tolist()\",\n",
       " \"df.groupby('Job Title')['avg_salary'].mean().nlargest(4).index.tolist()\",\n",
       " \"df.nsmallest(3, 'Founded')['Founded'].tolist()\",\n",
       " \"df['Rating'].nlargest(3).tolist()\",\n",
       " \"df.nlargest(6, 'Founded')['avg_salary'].tolist()\",\n",
       " \"df['user_followers_count'] > df['user_following_count']).any()\",\n",
       " \"df['replies'] > df['retweets'].any()\",\n",
       " \"df[(df['user_verified'] == True) & (df['lang'] != 'en')].any()\",\n",
       " \"df['image_links'].notnull().any()\",\n",
       " \"df['author_id<gx:category>'].nunique()\",\n",
       " \"df['retweets'].max()\",\n",
       " \"df['favorites'].mean()\",\n",
       " \"df[df['user_verified'] == True]['user_tweets_count'].sum()\",\n",
       " \"df.groupby('author_name<gx:category>')['user_tweets_count<gx:number>'].max().idxmax()\",\n",
       " \"df['lang'].mode()[0]\",\n",
       " \"df['source'].mode().iloc[0]\",\n",
       " \"df['type'].value_counts().idxmax()\",\n",
       " \"df.nlargest(3, 'user_followers_count')['author_id'].tolist()\",\n",
       " \"df['mention_names'].explode().value_counts().nlargest(5).index.tolist()\",\n",
       " \"df['lang'].value_counts().nlargest(4).index.tolist()\",\n",
       " \"df['user_followers_count'].nlargest(3).tolist()\",\n",
       " \"df.nsmallest(4, 'user_favourites_count')['user_favourites_count'].tolist()\",\n",
       " \"df['retweets'].nlargest(6).tolist()\",\n",
       " \"df.nsmallest(5, 'replies')['replies'].tolist()\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_dataset['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(examples):\n",
    "    inputs = [f\"Question: {q} Answer: {a}\" for q, a in zip(examples['question'], examples['answer'])]\n",
    "\n",
    "    return tokenizer(inputs, padding='max_length', truncation=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the eos_token as pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = train_dataset.map(preprocess_text, batched=True)\n",
    "# val_tokenized = val_dataset.map(prepocess_text, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 20.23 GB, other allocations: 464.00 KB, max allowed: 20.40 GB). Tried to allocate 1.96 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_empty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1192\u001b[0m, in \u001b[0;36mModule.to_empty\u001b[0;34m(self, device, recurse)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_empty\u001b[39m(\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;28mself\u001b[39m: T, \u001b[38;5;241m*\u001b[39m, device: Optional[DeviceLikeType], recurse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move the parameters and buffers to the specified device without copying storage.\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \n\u001b[1;32m   1183\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecurse\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 900 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1193\u001b[0m, in \u001b[0;36mModule.to_empty.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_empty\u001b[39m(\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;28mself\u001b[39m: T, \u001b[38;5;241m*\u001b[39m, device: Optional[DeviceLikeType], recurse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move the parameters and buffers to the specified device without copying storage.\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \n\u001b[1;32m   1183\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\n\u001b[0;32m-> 1193\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, recurse\u001b[38;5;241m=\u001b[39mrecurse\n\u001b[1;32m   1194\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 20.23 GB, other allocations: 464.00 KB, max allowed: 20.40 GB). Tried to allocate 1.96 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "model = model.to_empty(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehran/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy='no',\n",
    "    eval_steps=500,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nt/4p7p37qd0pgcmrtb22v297r40000gn/T/ipykernel_86034/3106050419.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 20.23 GB, other allocations: 464.00 KB, max allowed: 20.40 GB). Tried to allocate 224.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# eval_dataset=val_dataset,   \u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/transformers/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    573\u001b[0m ):\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/transformers/trainer.py:846\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 846\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 900 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/CodeSpaces/Testing/Table_Answering/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 20.23 GB, other allocations: 464.00 KB, max allowed: 20.40 GB). Tried to allocate 224.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=val_dataset,   \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4).to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    for batch in train_dataloader:\n",
    "        inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        labels = inputs[\"input_ids\"].to(device)\n",
    "        outputs = model(**inputs, labels=labels).to(device)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model and Tokenizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "TODO: Add Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\")  # Use MPS\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_model\").to(device)\n",
    "\n",
    "# Generate text\n",
    "input_text = \"Your prompt here\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
